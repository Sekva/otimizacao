%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Relatório Final - Projeto de Pesquisa
% Métodos de Otimização
% Baltz & Machado
% Capítulo 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{\Large{Métodos Matemáticos de Otimização}}\label{chp:1}


\section{{O Conceito de Otimização}}

\hspace{0.8cm}
Diz-se otimização, o processo que tem como objetivo encontrar condições que
minimizam ou maximizam algo (seja energia, tempo, dinheiro, etc). Sendo este,
muitas vezes um trabalho árduo, custoso.

Dessa maneira, na matemática, este processo é amplamente utilizado quando
busca-se valores pertencentes ao conjunto \textit{A} (que pode ter
restrições), com o objetivo de encontrar uma solução ótima, aplicando os valores
de \textit{A} em numa função objetivo predefinida.

Podendo assim, serem representadas da seguinte forma:

    Dada a função
        \begin{equation}
            f : A \rightarrow \mathbb{R}
        \end{equation}

        \begin{itemize}
                \item Maximização pode ser definida como:
        \end{itemize}

                busca pelo elemento \(x_0 \in A\), que satisfaz:

                    \begin{equation}
                        f(x_0) \geq f(x);
                    \end{equation}

                para todo \(x \in A\).

        \begin{itemize}
                \item Minimização pode ser definida como:
        \end{itemize}

                busca pelo elemento \(x_0 \in A\), que satisfaz:

                    \begin{equation}
                        (x_0) \leq f(x);
                    \end{equation}

                para todo \(x \in A\).

\vspace{\baselineskip}
Com isso, podemos agora entender como esse processo pode ser custoso. Iniciando
com fato de que existem os pontos máximos e mínimos (pontos críticos), locais
e globais no espaço das funções. Sendo os pontos críticos locais, aqueles que
não são os menores ou maiores valores para a minimização e maximização,
respectivamente. E os pontos globais, aqueles que representam o menor ou maior
valor no espaço da função, para a minimização e maximização, respectivamente.

Criando assim, uma certa incerteza ao encontrar um valor crítico numa função,
já que é estritamente difícil saber se o ponto crítico encontrado é local ou
global.Como pode-se perceber na Figura
\ref{grafico_local_global_pontosCriticos}.


\begin{figure}[h]
    \includegraphics[width=0.43\textwidth]
        {src/grafico_local_global_pontosCriticos.png}
    \centering
    \caption{Exemplo de pontos críticos locais e globais indicados no gráfico
        de uma função}
    \label{grafico_local_global_pontosCriticos}
\end{figure}


\section{{Otimização de Funções à Uma variável real}}

\hspace{0.8cm}

Evidente que funções possuem as variáveis dependentes (a qual representa o
objeto da otimização) e variáveis independentes (cujo suas grandezas podem ser
selecionadas), podemos denotar que, para a função

\begin{equation}
	y = f(x),
\end{equation}
quando buscamos otimizá-la, temos como objetivo encontrar valores que quando
aplicados à \textit{x}, temos o mínimo ou máximo valor \textit{y} (seja ele
local, ou preferencialmente global).

Partindo dessa perspectiva, acaba surgindo a necessidade de utilizar algum
recurso para encontrar o os pontos críticos. E nesse sentindo, pode-se utilizar
a técnica de \textbf{derivação}, donde, oferece como recurso a possibilidade de
identificar tais pontos.

A derivada, é a representação da taxa de variação de uma função em relação a
um ponto da mesma. A partir dai podemos observar um fato interessante sobre
minimos e maximos. Observando, por exemplo,o caso do maximo, que quando a função
está num maximo, existem duas possibilidades, a primeira sendo que a função para
de crescer e a partir dai ela se torna indefinida, e a segunda possibilidade
quando a função para de crescer e começa a decrescer. De toda forma a função
para de crescer, e por definição, quando a derivada (taxa de variação) é
positiva, a função cresce, e quando negativa a função decresce. Dai podemos
concluir que quando a derivada representando a taxa de variação é zero,a função
ou para de crescer ou de decrescer, sendo assim um ponto de maximo ou de minimo.

Com o uso da derivada, podemos pensar num metodo de otimização bastante simples,
considerando \(f(x)\) a função que queremos otimizar e \(f'(x)\) sua função
derivada, podemos dizer que o conjunto $O$ possui todos os otimos locais ou não
de \(f(x)\):

\begin{equation}
    O := \{x | f'(x) = 0\}
\end{equation}


Portanto, aplicando um filtro em $O$ para obeter o maximo e minimo do conjunto,
acabamos por obeter o maximo e minimo de \(f(x)\):


\begin{equation}
    max(f(x)) = max(O)
\end{equation}

\begin{equation}
    min(f(x)) = min(O)
\end{equation}


Mas então podemos perceber dois problemas, determinar como encontrar os pontos
onde a derivada se anule, e determinar se temos de fato todos os pontos.

Determinar se temos todos os pontos criticos (pontos em que a derivida se anula)
é um problema que podemos adiar por agora. Já encontrar os pontos pontos
criticos não é tão dificil, considerando boas aproximações.

O Método de Newton nos diz que uma sequencia \(\{x_k\}\) converge para o minimo
de \(f(x)\):

\begin{equation}
    x_{k+1} = x_{k} - \frac {f'(x_{k})}{f''(x_{k})}
\end{equation}

O que este método faz de fato é encontrar zeros de uma função \(g(x)\) a partir
da mesma sequencia \(\{x_k\}\), afim de que \(x_k\) converga para uma entrada
\(x^*\) de \(g(x)\), tal que a mesma se anule:

\begin{equation}
    x_{k+1} = x_{k} - \frac {g(x_{k})}{g'(x_{k})}
\end{equation}

\begin{equation}
    g(x^*) = 0
\end{equation}


Com isso temos um método que minimiza uma função que pode ser derivada duas
vezes (caso não possa, aproximações de suas primeiras e segundas derivadas
podem ser boas o suficiente) a partir de um valor de entrada. O método é
simples, entrega muitas vezes otimos locais proximos ao ponto inicial, mas tem
seu destaque pode ser curto e facilmente computavel.

Problemas de maximização podem ser vistos sob o seguinte olhar:

\begin{equation}
    max(f(x)) = min(-1 * f(x))
\end{equation}

Que com isto podem ser otimizados pelo Método de Newton também.

O movimento de \(x_k\) dentro da sequencia, é determinado pela relação das
quantidades e propriedades que tanto a primeira quanto a segunda derivada
oferecem. As quantidades determinam a velocidade do motivento e os sinais a
direção do movimento. De certa forma podemos ver esse movimento da sequencia
\(\{x_k\}\) como instantes do movimento de uma bola numa ladeira, que no começo
de sua descida é acelerada, e, conforme chega ao plano no fim da ladeira, começa
a reduzir sua velocidade, até supostamente chegar no ponto mais baixo descendo a
tal ladeira.

\section{{Programando o Método}}

\hspace{0.8cm}

A forma mais simples e mais util de implementar o metodo de Newton é na forma
de busca das raizes, que uma vez  implementada, só precisamos por como entrada
a priemira e a segunda derivada da função que desejamos minimizar, já que o
método não precisa saber qual a função de fato. A seguir temos a implementação
na liguagem Rust:

\input{./projetos/rust_newton1x1.tex}

Os parametros da função são:

    \begin{itemize}
            \item Uma função \(f : \mathbb{R} \rightarrow \mathbb{R}\)
            \item Uma entrada x sendo o chute inicial do otimo.
    \end{itemize}


A função derive1x1 é um função que calcula numericamente a derivada da função
passada como argumento no ponto também passado como argumento. Esta função é
apenas para funções \(f : \mathbb{R} \rightarrow \mathbb{R}\) e deve ser escrita
pelo úsuario como bem entender.



\textcolor[rgb]{1,0,0}{\section{{Otimização de Funções à Várias Variáveis}}}

\hspace{0.8cm}





%
